{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SAM 2: Segment Anything in Images and Videos - Colab Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sarptandoven/SAM2-SARP-DEMO/blob/main/sam2_colab_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use **SAM 2** (Segment Anything Model 2) for image and video segmentation tasks. SAM 2 is a foundation model that can segment any object in images and videos using prompts like clicks or bounding boxes.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Image segmentation with prompts\n",
        "- ‚úÖ Video segmentation and tracking\n",
        "- ‚úÖ Automatic mask generation\n",
        "- ‚úÖ Multiple model sizes (tiny, small, base+, large)\n",
        "- ‚úÖ Optimized for Colab with GPU acceleration\n",
        "\n",
        "## Model Sizes:\n",
        "- **sam2.1_hiera_tiny**: 38.9M parameters, fastest inference\n",
        "- **sam2.1_hiera_small**: 46M parameters, good balance\n",
        "- **sam2.1_hiera_base_plus**: 80.8M parameters, better accuracy\n",
        "- **sam2.1_hiera_large**: 224.4M parameters, best accuracy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Setup and Installation\n",
        "\n",
        "Let's start by setting up the environment and installing all required dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we're running in Colab\n",
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üî• Running in Google Colab - GPU enabled!\")\n",
        "else:\n",
        "    print(\"üíª Running locally\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install/upgrade PyTorch and torchvision to required versions\n",
        "%pip install torch>=2.5.1 torchvision>=0.20.1 --upgrade --quiet\n",
        "\n",
        "# Install other required packages\n",
        "%pip install numpy>=1.24.4 tqdm>=4.66.1 hydra-core>=1.3.2 iopath>=0.1.10 pillow>=9.4.0 --quiet\n",
        "\n",
        "# Install additional packages for notebooks\n",
        "%pip install matplotlib>=3.9.1 opencv-python>=4.7.0 eva-decord>=0.6.1 --quiet\n",
        "\n",
        "# Install packages for video processing\n",
        "%pip install av imageio-ffmpeg --quiet\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the SAM2 repository\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "# Remove existing sam2 directory if it exists\n",
        "if os.path.exists('sam2'):\n",
        "    shutil.rmtree('sam2')\n",
        "\n",
        "# Clone the repository\n",
        "result = subprocess.run(['git', 'clone', 'https://github.com/facebookresearch/sam2.git'], \n",
        "                       capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    print(f\"Error cloning repository: {result.stderr}\")\n",
        "    exit(1)\n",
        "\n",
        "# Change to the sam2 directory\n",
        "os.chdir('sam2')\n",
        "\n",
        "print(\"‚úÖ SAM2 repository cloned successfully!\")\n",
        "print(f\"üìÇ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install SAM2 in development mode\n",
        "# Skip CUDA extension build in Colab to avoid issues\n",
        "os.environ['SAM2_BUILD_CUDA'] = '0'\n",
        "\n",
        "%pip install -e \".[notebooks]\" --quiet\n",
        "\n",
        "print(\"‚úÖ SAM2 installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  Note: CUDA extension disabled for Colab compatibility\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model checkpoints\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create checkpoints directory\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Model checkpoints URLs (SAM 2.1)\n",
        "CHECKPOINTS = {\n",
        "    'sam2.1_hiera_tiny.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt',\n",
        "    'sam2.1_hiera_small.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt',\n",
        "    'sam2.1_hiera_base_plus.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt',\n",
        "    'sam2.1_hiera_large.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt'\n",
        "}\n",
        "\n",
        "def download_checkpoint(filename, url):\n",
        "    \"\"\"Download a model checkpoint with progress bar.\"\"\"\n",
        "    filepath = f'checkpoints/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"‚úÖ {filename} already exists\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üì• Downloading {filename}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    \n",
        "    with open(filepath, 'wb') as f, tqdm(\n",
        "        desc=filename,\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as pbar:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "# Download the small model by default (good balance of speed and accuracy)\n",
        "download_checkpoint('sam2.1_hiera_small.pt', CHECKPOINTS['sam2.1_hiera_small.pt'])\n",
        "\n",
        "# Optionally download other models (uncomment as needed)\n",
        "# download_checkpoint('sam2.1_hiera_tiny.pt', CHECKPOINTS['sam2.1_hiera_tiny.pt'])\n",
        "# download_checkpoint('sam2.1_hiera_base_plus.pt', CHECKPOINTS['sam2.1_hiera_base_plus.pt'])\n",
        "# download_checkpoint('sam2.1_hiera_large.pt', CHECKPOINTS['sam2.1_hiera_large.pt'])\n",
        "\n",
        "print(\"\\\\n‚úÖ Model checkpoints ready!\")\n",
        "print(f\"üìÇ Checkpoints directory: {os.path.abspath('checkpoints')}\")\n",
        "print(\"üìù Downloaded models:\")\n",
        "for f in os.listdir('checkpoints'):\n",
        "    if f.endswith('.pt'):\n",
        "        size = os.path.getsize(f'checkpoints/{f}') / (1024**2)\n",
        "        print(f\"  - {f} ({size:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Import Required Libraries\n",
        "\n",
        "Let's import all the necessary libraries for SAM2 usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SAM2 imports\n",
        "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Model Setup\n",
        "\n",
        "Let's set up the SAM2 model with the desired configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"sam2.1_hiera_small\"  # Change this to use different model sizes\n",
        "CHECKPOINT_PATH = f\"checkpoints/{MODEL_NAME}.pt\"\n",
        "CONFIG_PATH = f\"configs/sam2.1/{MODEL_NAME.replace('sam2.1_', 'sam2.1_')}.yaml\"\n",
        "\n",
        "# Check if files exist\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"‚ùå Checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "    print(\"Please download the checkpoint first using the cell above.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Checkpoint found: {CHECKPOINT_PATH}\")\n",
        "\n",
        "if not os.path.exists(CONFIG_PATH):\n",
        "    print(f\"‚ùå Config not found: {CONFIG_PATH}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Config found: {CONFIG_PATH}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üîß Using device: {device}\")\n",
        "\n",
        "# Set up model for image prediction\n",
        "sam2_model = build_sam2(CONFIG_PATH, CHECKPOINT_PATH, device=device)\n",
        "image_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "# Set up automatic mask generator\n",
        "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)\n",
        "\n",
        "print(f\"‚úÖ SAM2 model ({MODEL_NAME}) loaded successfully!\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in sam2_model.parameters()) / 1e6:.1f}M\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ†Ô∏è Helper Functions\n",
        "\n",
        "Let's define some useful helper functions for visualization and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_url(url):\n",
        "    \"\"\"Load an image from a URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    return np.array(image)\n",
        "\n",
        "def load_image_from_path(path):\n",
        "    \"\"\"Load an image from a local path.\"\"\"\n",
        "    image = Image.open(path)\n",
        "    return np.array(image)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False, alpha=0.5):\n",
        "    \"\"\"Display a mask on the given axes.\"\"\"\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), [alpha]])\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, alpha])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=200):\n",
        "    \"\"\"Display points on the given axes.\"\"\"\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    \"\"\"Display a bounding box on the given axes.\"\"\"\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_anns(anns, ax):\n",
        "    \"\"\"Display automatic mask annotations.\"\"\"\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    \n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "def download_sample_image(url, filename):\n",
        "    \"\"\"Download a sample image for testing.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"üì• Downloading sample image: {filename}\")\n",
        "        response = requests.get(url)\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"‚úÖ Sample image downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Sample image already exists: {filename}\")\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üñºÔ∏è Image Segmentation with Prompts\n",
        "\n",
        "Let's try image segmentation with different types of prompts (clicks, boxes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download sample images for demonstration\n",
        "SAMPLE_IMAGES = {\n",
        "    'truck.jpg': 'https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/truck.jpg',\n",
        "    'groceries.jpg': 'https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/groceries.jpg',\n",
        "}\n",
        "\n",
        "# Create images directory\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# Download sample images\n",
        "for filename, url in SAMPLE_IMAGES.items():\n",
        "    download_sample_image(url, f'images/{filename}')\n",
        "\n",
        "print(\"\\\\nüìÇ Available sample images:\")\n",
        "for f in os.listdir('images'):\n",
        "    if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and segment an image with point prompts\n",
        "image_path = 'images/truck.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define prompts (x, y coordinates)\n",
        "input_points = np.array([[500, 375]])  # Click on truck\n",
        "input_labels = np.array([1])  # 1 = positive click, 0 = negative click\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        multimask_output=True,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "# Original image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Show masks\n",
        "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    axes[i+1].imshow(image)\n",
        "    show_mask(mask, axes[i+1])\n",
        "    show_points(input_points, input_labels, axes[i+1])\n",
        "    axes[i+1].set_title(f'Mask {i+1} (Score: {score:.3f})')\n",
        "    axes[i+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated {len(masks)} masks with scores: {scores}\")\n",
        "print(f\"üéØ Best mask index: {np.argmax(scores)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment with bounding box prompt\n",
        "image_path = 'images/groceries.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define bounding box [x1, y1, x2, y2]\n",
        "input_box = np.array([425, 600, 700, 875])  # Bounding box around an object\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Original image with bounding box\n",
        "axes[0].imshow(image)\n",
        "show_box(input_box, axes[0])\n",
        "axes[0].set_title('Original Image with Bounding Box')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Segmented result\n",
        "axes[1].imshow(image)\n",
        "show_mask(masks[0], axes[1])\n",
        "show_box(input_box, axes[1])\n",
        "axes[1].set_title(f'Segmentation Result (Score: {scores[0]:.3f})')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated mask with score: {scores[0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine multiple prompts for better segmentation\n",
        "image_path = 'images/truck.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define multiple prompts\n",
        "input_points = np.array([[500, 375], [600, 400]])  # Multiple positive clicks\n",
        "input_labels = np.array([1, 1])  # Both positive\n",
        "input_box = np.array([425, 300, 700, 500])  # Bounding box\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Original image with prompts\n",
        "axes[0].imshow(image)\n",
        "show_points(input_points, input_labels, axes[0])\n",
        "show_box(input_box, axes[0])\n",
        "axes[0].set_title('Original Image with Combined Prompts')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Segmented result\n",
        "axes[1].imshow(image)\n",
        "show_mask(masks[0], axes[1])\n",
        "show_points(input_points, input_labels, axes[1])\n",
        "show_box(input_box, axes[1])\n",
        "axes[1].set_title(f'Combined Prompts Result (Score: {scores[0]:.3f})')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated mask with combined prompts, score: {scores[0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Automatic Mask Generation\n",
        "\n",
        "Let's try automatic mask generation to segment everything in the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automatic mask generation\n",
        "image_path = 'images/groceries.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "print(\"üîÑ Generating automatic masks... This may take a moment.\")\n",
        "\n",
        "# Generate masks automatically\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks = mask_generator.generate(image)\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Original image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Automatic segmentation\n",
        "axes[1].imshow(image)\n",
        "show_anns(masks, axes[1])\n",
        "axes[1].set_title(f'Automatic Segmentation ({len(masks)} masks)')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated {len(masks)} automatic masks\")\n",
        "print(f\"üìä Mask areas: {[m['area'] for m in masks[:5]][:5]}...\")  # Show first 5 areas\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "Congratulations! You've successfully set up and used SAM2 in Google Colab. Here's what you've learned:\n",
        "\n",
        "‚úÖ **Setup**: How to install SAM2 and its dependencies in Colab  \n",
        "‚úÖ **Image Segmentation**: Using point and box prompts for precise segmentation  \n",
        "‚úÖ **Automatic Masks**: Generating all possible masks automatically  \n",
        "‚úÖ **Helper Functions**: Building custom visualization and processing tools  \n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. **Experiment** with your own images\n",
        "2. **Try different models** to find the best balance for your use case\n",
        "3. **Combine prompts** for more precise segmentation\n",
        "4. **Build applications** using SAM2 as a foundation\n",
        "5. **Explore fine-tuning** for domain-specific tasks\n",
        "\n",
        "### üìö Resources:\n",
        "\n",
        "- [SAM2 Paper](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)\n",
        "- [GitHub Repository](https://github.com/facebookresearch/sam2)\n",
        "- [SAM2 Demo](https://sam2.metademolab.com/)\n",
        "- [Hugging Face Models](https://huggingface.co/models?search=facebook/sam2)\n",
        "\n",
        "### üí° Tips for Better Results:\n",
        "\n",
        "1. **Point Prompts**: Use positive points (label=1) to include areas, negative points (label=0) to exclude\n",
        "2. **Bounding Boxes**: Draw tight boxes around objects for better segmentation\n",
        "3. **Combined Prompts**: Mix points and boxes for optimal results\n",
        "4. **Model Selection**: Use tiny for speed, large for accuracy\n",
        "5. **Image Quality**: Higher resolution images generally give better results\n",
        "\n",
        "### üîß Troubleshooting:\n",
        "\n",
        "- If you encounter memory issues, try using a smaller model or reducing image resolution\n",
        "- For CUDA extension issues, the notebook is configured to skip them (functionality will be slightly limited but still work)\n",
        "- If segmentation quality is poor, try adjusting prompt placement or using multiple prompts\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Segmenting! üéØ**\n",
        "\n",
        "Upload your own images and experiment with different prompts to see SAM2 in action!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
