{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarptandoven/sam2-demo/blob/main/SAM2_Colab_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üé¨ SAM2 Video Segmentation Demo on Google Colab\n",
        "\n",
        "**Segment Anything 2 (SAM 2)** - Interactive video object segmentation and tracking demo running on GPU with a web interface.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Interactive video object segmentation\n",
        "- ‚úÖ Real-time object tracking across frames\n",
        "- ‚úÖ Web-based UI with video effects\n",
        "- ‚úÖ Video upload support (up to 100MB)\n",
        "- ‚úÖ Export segmented videos\n",
        "- ‚úÖ GPU acceleration for fast inference\n",
        "\n",
        "**‚ö†Ô∏è Important:** Make sure to use a GPU runtime for optimal performance!\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Setup Instructions\n",
        "\n",
        "1. **Enable GPU**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU` (T4, V100, or A100)\n",
        "2. **Run all cells**: Execute each cell in order\n",
        "3. **Access the demo**: Click on the generated public URL at the end\n",
        "\n",
        "‚è±Ô∏è **Estimated setup time**: 3-5 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Check GPU availability and system info\n",
        "import torch\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"üñ•Ô∏è  System Information:\")\n",
        "print(f\"   Python version: {torch.__version__}\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(\"   ‚úÖ GPU is ready for SAM2!\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No GPU detected. Please enable GPU runtime for optimal performance.\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")\n",
        "\n",
        "# Check available disk space\n",
        "result = subprocess.run(['df', '-h', '/content'], capture_output=True, text=True)\n",
        "print(f\"\\nüíæ Available disk space:\")\n",
        "print(result.stdout.split('\\n')[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì• Clone the SAM2 demo repository\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/sam2-demo'):\n",
        "    print(\"üîÑ Repository already exists, pulling latest changes...\")\n",
        "    %cd /content/sam2-demo\n",
        "    !git pull origin main\n",
        "else:\n",
        "    print(\"üì• Cloning SAM2 demo repository...\")\n",
        "    %cd /content\n",
        "    !git clone https://github.com/sarptandoven/sam2-demo.git\n",
        "    %cd sam2-demo\n",
        "\n",
        "print(\"‚úÖ Repository ready!\")\n",
        "!pwd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üì¶ Install system dependencies\n",
        "print(\"üì¶ Installing system dependencies...\")\n",
        "\n",
        "# Update package list and install required packages\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq \\\n",
        "    ffmpeg \\\n",
        "    libsm6 \\\n",
        "    libxext6 \\\n",
        "    libxrender-dev \\\n",
        "    libglib2.0-0 \\\n",
        "    libgl1-mesa-glx \\\n",
        "    curl\n",
        "\n",
        "# Install Node.js and Yarn for frontend\n",
        "print(\"üì¶ Installing Node.js and Yarn...\")\n",
        "!curl -fsSL https://deb.nodesource.com/setup_18.x | bash -\n",
        "!apt-get install -y nodejs\n",
        "!npm install -g yarn\n",
        "\n",
        "# Verify installations\n",
        "!echo \"Node version: $(node --version)\"\n",
        "!echo \"Yarn version: $(yarn --version)\"\n",
        "!echo \"FFmpeg version: $(ffmpeg -version | head -1)\"\n",
        "\n",
        "print(\"‚úÖ System dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üêç Install Python dependencies\n",
        "print(\"üêç Installing Python dependencies...\")\n",
        "\n",
        "# Install core dependencies\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q \\\n",
        "    opencv-python \\\n",
        "    Pillow \\\n",
        "    numpy \\\n",
        "    matplotlib \\\n",
        "    imagesize \\\n",
        "    dataclasses-json \\\n",
        "    pycocotools \\\n",
        "    av \\\n",
        "    flask \\\n",
        "    flask-cors \\\n",
        "    strawberry-graphql \\\n",
        "    pydantic \\\n",
        "    python-multipart\n",
        "\n",
        "# Install SAM2 package\n",
        "print(\"üîß Installing SAM2 package...\")\n",
        "!pip install -q -e .\n",
        "\n",
        "print(\"‚úÖ Python dependencies installed!\")\n",
        "\n",
        "# Verify PyTorch CUDA\n",
        "import torch\n",
        "print(f\"üî• PyTorch CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ü§ñ Download SAM2 model checkpoints\n",
        "print(\"ü§ñ Downloading SAM2 model checkpoints...\")\n",
        "\n",
        "import os\n",
        "os.makedirs('/content/sam2-demo/checkpoints', exist_ok=True)\n",
        "\n",
        "# Download model checkpoints\n",
        "%cd /content/sam2-demo/checkpoints\n",
        "!bash download_ckpts.sh\n",
        "\n",
        "# Verify downloads\n",
        "print(\"\\nüìã Downloaded checkpoints:\")\n",
        "!ls -lh *.pt\n",
        "\n",
        "print(\"‚úÖ Model checkpoints ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé® Install frontend dependencies\n",
        "print(\"üé® Installing frontend dependencies...\")\n",
        "\n",
        "%cd /content/sam2-demo/demo/frontend\n",
        "!yarn install --legacy-peer-deps\n",
        "\n",
        "print(\"‚úÖ Frontend dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß Setup environment variables and configuration\n",
        "import os\n",
        "\n",
        "# Set environment variables for GPU usage\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['APP_ROOT'] = '/content/sam2-demo'\n",
        "os.environ['DATA_PATH'] = '/content/sam2-demo/demo/data'\n",
        "os.environ['API_URL'] = 'http://localhost:7263'\n",
        "os.environ['MODEL_SIZE'] = 'small'  # Use small model for faster inference\n",
        "os.environ['DEFAULT_VIDEO_PATH'] = 'gallery/01_dog.mp4'\n",
        "os.environ['PYTHONPATH'] = '/content/sam2-demo:' + os.environ.get('PYTHONPATH', '')\n",
        "\n",
        "print(\"üîß Environment configured:\")\n",
        "print(f\"   - App root: {os.environ['APP_ROOT']}\")\n",
        "print(f\"   - Model size: {os.environ['MODEL_SIZE']}\")\n",
        "print(f\"   - API URL: {os.environ['API_URL']}\")\n",
        "print(f\"   - CUDA device: {os.environ.get('CUDA_VISIBLE_DEVICES', 'default')}\")\n",
        "\n",
        "%cd /content/sam2-demo\n",
        "print(\"‚úÖ Environment ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåê Install Cloudflare Tunnel for public access\n",
        "print(\"üåê Installing Cloudflare Tunnel for public access...\")\n",
        "\n",
        "# Download and install cloudflared\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared-linux-amd64.deb\n",
        "\n",
        "print(\"‚úÖ Cloudflare Tunnel installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üöÄ Start the backend server\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"üöÄ Starting SAM2 backend server...\")\n",
        "\n",
        "# Change to backend directory\n",
        "backend_dir = '/content/sam2-demo/demo/backend/server'\n",
        "os.chdir(backend_dir)\n",
        "\n",
        "# Start backend server in background\n",
        "backend_cmd = ['python', 'app.py']\n",
        "\n",
        "# Set environment for backend\n",
        "backend_env = os.environ.copy()\n",
        "backend_env.update({\n",
        "    'PYTORCH_ENABLE_MPS_FALLBACK': '1',\n",
        "    'APP_ROOT': '/content/sam2-demo',\n",
        "    'DATA_PATH': '/content/sam2-demo/demo/data',\n",
        "    'API_URL': 'http://localhost:7263',\n",
        "    'MODEL_SIZE': 'small',\n",
        "    'DEFAULT_VIDEO_PATH': 'gallery/01_dog.mp4',\n",
        "    'PYTHONPATH': '/content/sam2-demo'\n",
        "})\n",
        "\n",
        "# Start backend process\n",
        "backend_process = subprocess.Popen(\n",
        "    backend_cmd,\n",
        "    env=backend_env,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "print(\"‚è≥ Waiting for backend to initialize...\")\n",
        "\n",
        "# Monitor backend startup\n",
        "for i in range(60):  # Wait up to 60 seconds\n",
        "    if backend_process.poll() is not None:\n",
        "        print(\"‚ùå Backend process exited unexpectedly\")\n",
        "        break\n",
        "    \n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get('http://localhost:7263/graphql', timeout=2)\n",
        "        if response.status_code == 405:  # GraphQL expects POST, 405 = Method Not Allowed is fine\n",
        "            print(\"‚úÖ Backend server is running on port 7263\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    time.sleep(1)\n",
        "    if i % 10 == 0:\n",
        "        print(f\"   ... still waiting ({i}s)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Backend startup timeout, but continuing...\")\n",
        "\n",
        "print(\"üéØ Backend process started (PID: {})\".format(backend_process.pid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üé® Start the frontend server\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "print(\"üé® Starting frontend server...\")\n",
        "\n",
        "# Change to frontend directory\n",
        "frontend_dir = '/content/sam2-demo/demo/frontend'\n",
        "os.chdir(frontend_dir)\n",
        "\n",
        "# Start frontend server in background\n",
        "frontend_cmd = ['yarn', 'dev', '--host', '0.0.0.0', '--port', '5173']\n",
        "\n",
        "frontend_process = subprocess.Popen(\n",
        "    frontend_cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "print(\"‚è≥ Waiting for frontend to start...\")\n",
        "\n",
        "# Monitor frontend startup\n",
        "for i in range(30):  # Wait up to 30 seconds\n",
        "    if frontend_process.poll() is not None:\n",
        "        print(\"‚ùå Frontend process exited unexpectedly\")\n",
        "        break\n",
        "    \n",
        "    try:\n",
        "        import requests\n",
        "        response = requests.get('http://localhost:5173', timeout=2)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úÖ Frontend server is running on port 5173\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    time.sleep(1)\n",
        "    if i % 5 == 0:\n",
        "        print(f\"   ... still waiting ({i}s)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Frontend startup timeout, but continuing...\")\n",
        "\n",
        "print(\"üéØ Frontend process started (PID: {})\".format(frontend_process.pid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üåê Create public tunnel and get URL\n",
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "import threading\n",
        "\n",
        "print(\"üåê Creating public tunnel to frontend...\")\n",
        "\n",
        "# Start cloudflared tunnel\n",
        "tunnel_cmd = ['cloudflared', 'tunnel', '--url', 'http://localhost:5173']\n",
        "\n",
        "tunnel_process = subprocess.Popen(\n",
        "    tunnel_cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    universal_newlines=True\n",
        ")\n",
        "\n",
        "# Function to read tunnel output and extract URL\n",
        "public_url = None\n",
        "\n",
        "def read_tunnel_output():\n",
        "    global public_url\n",
        "    for line in iter(tunnel_process.stdout.readline, ''):\n",
        "        if 'trycloudflare.com' in line:\n",
        "            match = re.search(r'https://[\\w-]+\\.trycloudflare\\.com', line)\n",
        "            if match:\n",
        "                public_url = match.group(0)\n",
        "                break\n",
        "\n",
        "# Start reading tunnel output in background\n",
        "output_thread = threading.Thread(target=read_tunnel_output)\n",
        "output_thread.start()\n",
        "\n",
        "# Wait for URL\n",
        "print(\"‚è≥ Waiting for public URL...\")\n",
        "for i in range(20):\n",
        "    if public_url:\n",
        "        break\n",
        "    time.sleep(1)\n",
        "    if i % 5 == 0:\n",
        "        print(f\"   ... still waiting ({i}s)\")\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\nüéâ SAM2 Demo is ready!\")\n",
        "    print(f\"\\nüîó Public URL: {public_url}\")\n",
        "    print(f\"\\nüì± Click the link above to access the demo\")\n",
        "    print(f\"\\n‚ö° Features available:\")\n",
        "    print(f\"   - Interactive video segmentation\")\n",
        "    print(f\"   - Real-time object tracking\")\n",
        "    print(f\"   - Video upload (up to 100MB)\")\n",
        "    print(f\"   - Video effects and export\")\n",
        "    print(f\"   - GPU-accelerated inference\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to get public URL. You can still access locally at http://localhost:5173\")\n",
        "\n",
        "print(f\"\\nüéØ Tunnel process started (PID: {tunnel_process.pid})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üìä Monitor services and show status\n",
        "import requests\n",
        "import time\n",
        "import torch\n",
        "\n",
        "def check_service_status():\n",
        "    print(\"üìä Service Status Check:\")\n",
        "    \n",
        "    # Check backend\n",
        "    try:\n",
        "        response = requests.get('http://localhost:7263/graphql', timeout=5)\n",
        "        backend_status = \"‚úÖ Running\" if response.status_code in [200, 405] else \"‚ùå Error\"\n",
        "    except:\n",
        "        backend_status = \"‚ùå Not responding\"\n",
        "    \n",
        "    # Check frontend\n",
        "    try:\n",
        "        response = requests.get('http://localhost:5173', timeout=5)\n",
        "        frontend_status = \"‚úÖ Running\" if response.status_code == 200 else \"‚ùå Error\"\n",
        "    except:\n",
        "        frontend_status = \"‚ùå Not responding\"\n",
        "    \n",
        "    # GPU info\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_memory = torch.cuda.memory_allocated() / 1e9\n",
        "        gpu_status = f\"‚úÖ GPU Memory: {gpu_memory:.1f}GB used\"\n",
        "    else:\n",
        "        gpu_status = \"‚ö†Ô∏è  No GPU available\"\n",
        "    \n",
        "    print(f\"   üñ•Ô∏è  Backend (GraphQL): {backend_status}\")\n",
        "    print(f\"   üé® Frontend (React): {frontend_status}\")\n",
        "    print(f\"   üî• {gpu_status}\")\n",
        "    \n",
        "    if 'public_url' in globals() and public_url:\n",
        "        print(f\"   üåê Public URL: {public_url}\")\n",
        "    \n",
        "    return backend_status, frontend_status\n",
        "\n",
        "# Initial status check\n",
        "check_service_status()\n",
        "\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"   - If services show as not responding, wait a moment and run this cell again\")\n",
        "print(\"   - The demo works best with videos under 10 seconds and 100MB\")\n",
        "print(\"   - Try the gallery videos first to test the interface\")\n",
        "print(\"   - Use the 'Add Object' button to start segmenting objects in videos\")\n",
        "\n",
        "print(\"\\nüîÑ Services will run until this notebook session ends.\")\n",
        "print(\"üìù Re-run this cell anytime to check service status.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîß Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "### Common Issues:\n",
        "1. **Services not responding**: Wait a moment and re-run the status check cell\n",
        "2. **GPU not detected**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `GPU`\n",
        "3. **Out of memory**: Try using a smaller model size (change `MODEL_SIZE` to `tiny`)\n",
        "4. **Video upload fails**: Ensure video is under 10 seconds and 100MB\n",
        "\n",
        "### Restart Services:\n",
        "If needed, you can restart the notebook by going to `Runtime` ‚Üí `Restart and run all`\n",
        "\n",
        "### Manual Service Control:\n",
        "- Backend logs: Check the backend process output for errors\n",
        "- Frontend logs: Check the frontend process output for build issues\n",
        "- GPU memory: Monitor GPU usage with the status check cell\n",
        "\n",
        "### Support:\n",
        "For issues, please check the [GitHub repository](https://github.com/sarptandoven/sam2-demo)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# SAM 2: Segment Anything in Images and Videos - Colab Demo\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sarptandoven/SAM2-SARP-DEMO/blob/main/sam2_colab_demo.ipynb)\n",
        "\n",
        "This notebook demonstrates how to use **SAM 2** (Segment Anything Model 2) for image and video segmentation tasks. SAM 2 is a foundation model that can segment any object in images and videos using prompts like clicks or bounding boxes.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Image segmentation with prompts\n",
        "- ‚úÖ Video segmentation and tracking\n",
        "- ‚úÖ Automatic mask generation\n",
        "- ‚úÖ Multiple model sizes (tiny, small, base+, large)\n",
        "- ‚úÖ Optimized for Colab with GPU acceleration\n",
        "\n",
        "## Model Sizes:\n",
        "- **sam2.1_hiera_tiny**: 38.9M parameters, fastest inference\n",
        "- **sam2.1_hiera_small**: 46M parameters, good balance\n",
        "- **sam2.1_hiera_base_plus**: 80.8M parameters, better accuracy\n",
        "- **sam2.1_hiera_large**: 224.4M parameters, best accuracy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üöÄ Setup and Installation\n",
        "\n",
        "Let's start by setting up the environment and installing all required dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if we're running in Colab\n",
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üî• Running in Google Colab - GPU enabled!\")\n",
        "else:\n",
        "    print(\"üíª Running locally\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install/upgrade PyTorch and torchvision to required versions\n",
        "%pip install torch>=2.5.1 torchvision>=0.20.1 --upgrade --quiet\n",
        "\n",
        "# Install other required packages\n",
        "%pip install numpy>=1.24.4 tqdm>=4.66.1 hydra-core>=1.3.2 iopath>=0.1.10 pillow>=9.4.0 --quiet\n",
        "\n",
        "# Install additional packages for notebooks\n",
        "%pip install matplotlib>=3.9.1 opencv-python>=4.7.0 eva-decord>=0.6.1 --quiet\n",
        "\n",
        "# Install packages for video processing\n",
        "%pip install av imageio-ffmpeg --quiet\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone the SAM2 repository\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "# Remove existing sam2 directory if it exists\n",
        "if os.path.exists('sam2'):\n",
        "    shutil.rmtree('sam2')\n",
        "\n",
        "# Clone the repository\n",
        "result = subprocess.run(['git', 'clone', 'https://github.com/facebookresearch/sam2.git'], \n",
        "                       capture_output=True, text=True)\n",
        "if result.returncode != 0:\n",
        "    print(f\"Error cloning repository: {result.stderr}\")\n",
        "    exit(1)\n",
        "\n",
        "# Change to the sam2 directory\n",
        "os.chdir('sam2')\n",
        "\n",
        "print(\"‚úÖ SAM2 repository cloned successfully!\")\n",
        "print(f\"üìÇ Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install SAM2 in development mode\n",
        "# Skip CUDA extension build in Colab to avoid issues\n",
        "os.environ['SAM2_BUILD_CUDA'] = '0'\n",
        "\n",
        "%pip install -e \".[notebooks]\" --quiet\n",
        "\n",
        "print(\"‚úÖ SAM2 installed successfully!\")\n",
        "print(\"‚ö†Ô∏è  Note: CUDA extension disabled for Colab compatibility\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download model checkpoints\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create checkpoints directory\n",
        "os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "# Model checkpoints URLs (SAM 2.1)\n",
        "CHECKPOINTS = {\n",
        "    'sam2.1_hiera_tiny.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt',\n",
        "    'sam2.1_hiera_small.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt',\n",
        "    'sam2.1_hiera_base_plus.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt',\n",
        "    'sam2.1_hiera_large.pt': 'https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt'\n",
        "}\n",
        "\n",
        "def download_checkpoint(filename, url):\n",
        "    \"\"\"Download a model checkpoint with progress bar.\"\"\"\n",
        "    filepath = f'checkpoints/{filename}'\n",
        "    if os.path.exists(filepath):\n",
        "        print(f\"‚úÖ {filename} already exists\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üì• Downloading {filename}...\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    \n",
        "    with open(filepath, 'wb') as f, tqdm(\n",
        "        desc=filename,\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as pbar:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            if chunk:\n",
        "                f.write(chunk)\n",
        "                pbar.update(len(chunk))\n",
        "\n",
        "# Download the small model by default (good balance of speed and accuracy)\n",
        "download_checkpoint('sam2.1_hiera_small.pt', CHECKPOINTS['sam2.1_hiera_small.pt'])\n",
        "\n",
        "# Optionally download other models (uncomment as needed)\n",
        "# download_checkpoint('sam2.1_hiera_tiny.pt', CHECKPOINTS['sam2.1_hiera_tiny.pt'])\n",
        "# download_checkpoint('sam2.1_hiera_base_plus.pt', CHECKPOINTS['sam2.1_hiera_base_plus.pt'])\n",
        "# download_checkpoint('sam2.1_hiera_large.pt', CHECKPOINTS['sam2.1_hiera_large.pt'])\n",
        "\n",
        "print(\"\\\\n‚úÖ Model checkpoints ready!\")\n",
        "print(f\"üìÇ Checkpoints directory: {os.path.abspath('checkpoints')}\")\n",
        "print(\"üìù Downloaded models:\")\n",
        "for f in os.listdir('checkpoints'):\n",
        "    if f.endswith('.pt'):\n",
        "        size = os.path.getsize(f'checkpoints/{f}') / (1024**2)\n",
        "        print(f\"  - {f} ({size:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìö Import Required Libraries\n",
        "\n",
        "Let's import all the necessary libraries for SAM2 usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SAM2 imports\n",
        "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
        "print(f\"üöÄ CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ü§ñ Model Setup\n",
        "\n",
        "Let's set up the SAM2 model with the desired configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"sam2.1_hiera_small\"  # Change this to use different model sizes\n",
        "CHECKPOINT_PATH = f\"checkpoints/{MODEL_NAME}.pt\"\n",
        "CONFIG_PATH = f\"configs/sam2.1/{MODEL_NAME.replace('sam2.1_', 'sam2.1_')}.yaml\"\n",
        "\n",
        "# Check if files exist\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"‚ùå Checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "    print(\"Please download the checkpoint first using the cell above.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Checkpoint found: {CHECKPOINT_PATH}\")\n",
        "\n",
        "if not os.path.exists(CONFIG_PATH):\n",
        "    print(f\"‚ùå Config not found: {CONFIG_PATH}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Config found: {CONFIG_PATH}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üîß Using device: {device}\")\n",
        "\n",
        "# Set up model for image prediction\n",
        "sam2_model = build_sam2(CONFIG_PATH, CHECKPOINT_PATH, device=device)\n",
        "image_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "# Set up automatic mask generator\n",
        "mask_generator = SAM2AutomaticMaskGenerator(sam2_model)\n",
        "\n",
        "print(f\"‚úÖ SAM2 model ({MODEL_NAME}) loaded successfully!\")\n",
        "print(f\"üìä Model parameters: {sum(p.numel() for p in sam2_model.parameters()) / 1e6:.1f}M\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üõ†Ô∏è Helper Functions\n",
        "\n",
        "Let's define some useful helper functions for visualization and processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_url(url):\n",
        "    \"\"\"Load an image from a URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content))\n",
        "    return np.array(image)\n",
        "\n",
        "def load_image_from_path(path):\n",
        "    \"\"\"Load an image from a local path.\"\"\"\n",
        "    image = Image.open(path)\n",
        "    return np.array(image)\n",
        "\n",
        "def show_mask(mask, ax, random_color=False, alpha=0.5):\n",
        "    \"\"\"Display a mask on the given axes.\"\"\"\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), [alpha]])\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, alpha])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=200):\n",
        "    \"\"\"Display points on the given axes.\"\"\"\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    \"\"\"Display a bounding box on the given axes.\"\"\"\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
        "\n",
        "def show_anns(anns, ax):\n",
        "    \"\"\"Display automatic mask annotations.\"\"\"\n",
        "    if len(anns) == 0:\n",
        "        return\n",
        "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "    \n",
        "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in sorted_anns:\n",
        "        m = ann['segmentation']\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)\n",
        "\n",
        "def download_sample_image(url, filename):\n",
        "    \"\"\"Download a sample image for testing.\"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"üì• Downloading sample image: {filename}\")\n",
        "        response = requests.get(url)\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"‚úÖ Sample image downloaded: {filename}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Sample image already exists: {filename}\")\n",
        "\n",
        "print(\"‚úÖ Helper functions defined!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üñºÔ∏è Image Segmentation with Prompts\n",
        "\n",
        "Let's try image segmentation with different types of prompts (clicks, boxes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download sample images for demonstration\n",
        "SAMPLE_IMAGES = {\n",
        "    'truck.jpg': 'https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/truck.jpg',\n",
        "    'groceries.jpg': 'https://raw.githubusercontent.com/facebookresearch/sam2/main/notebooks/images/groceries.jpg',\n",
        "}\n",
        "\n",
        "# Create images directory\n",
        "os.makedirs('images', exist_ok=True)\n",
        "\n",
        "# Download sample images\n",
        "for filename, url in SAMPLE_IMAGES.items():\n",
        "    download_sample_image(url, f'images/{filename}')\n",
        "\n",
        "print(\"\\\\nüìÇ Available sample images:\")\n",
        "for f in os.listdir('images'):\n",
        "    if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        print(f\"  - {f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and segment an image with point prompts\n",
        "image_path = 'images/truck.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define prompts (x, y coordinates)\n",
        "input_points = np.array([[500, 375]])  # Click on truck\n",
        "input_labels = np.array([1])  # 1 = positive click, 0 = negative click\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        multimask_output=True,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "# Original image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Show masks\n",
        "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "    axes[i+1].imshow(image)\n",
        "    show_mask(mask, axes[i+1])\n",
        "    show_points(input_points, input_labels, axes[i+1])\n",
        "    axes[i+1].set_title(f'Mask {i+1} (Score: {score:.3f})')\n",
        "    axes[i+1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated {len(masks)} masks with scores: {scores}\")\n",
        "print(f\"üéØ Best mask index: {np.argmax(scores)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Segment with bounding box prompt\n",
        "image_path = 'images/groceries.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define bounding box [x1, y1, x2, y2]\n",
        "input_box = np.array([425, 600, 700, 875])  # Bounding box around an object\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=None,\n",
        "        point_labels=None,\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Original image with bounding box\n",
        "axes[0].imshow(image)\n",
        "show_box(input_box, axes[0])\n",
        "axes[0].set_title('Original Image with Bounding Box')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Segmented result\n",
        "axes[1].imshow(image)\n",
        "show_mask(masks[0], axes[1])\n",
        "show_box(input_box, axes[1])\n",
        "axes[1].set_title(f'Segmentation Result (Score: {scores[0]:.3f})')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated mask with score: {scores[0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine multiple prompts for better segmentation\n",
        "image_path = 'images/truck.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "# Set the image in the predictor\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    image_predictor.set_image(image)\n",
        "\n",
        "# Define multiple prompts\n",
        "input_points = np.array([[500, 375], [600, 400]])  # Multiple positive clicks\n",
        "input_labels = np.array([1, 1])  # Both positive\n",
        "input_box = np.array([425, 300, 700, 500])  # Bounding box\n",
        "\n",
        "# Predict masks\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks, scores, logits = image_predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        box=input_box[None, :],\n",
        "        multimask_output=False,\n",
        "    )\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "# Original image with prompts\n",
        "axes[0].imshow(image)\n",
        "show_points(input_points, input_labels, axes[0])\n",
        "show_box(input_box, axes[0])\n",
        "axes[0].set_title('Original Image with Combined Prompts')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Segmented result\n",
        "axes[1].imshow(image)\n",
        "show_mask(masks[0], axes[1])\n",
        "show_points(input_points, input_labels, axes[1])\n",
        "show_box(input_box, axes[1])\n",
        "axes[1].set_title(f'Combined Prompts Result (Score: {scores[0]:.3f})')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated mask with combined prompts, score: {scores[0]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîÑ Automatic Mask Generation\n",
        "\n",
        "Let's try automatic mask generation to segment everything in the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Automatic mask generation\n",
        "image_path = 'images/groceries.jpg'\n",
        "image = load_image_from_path(image_path)\n",
        "\n",
        "print(\"üîÑ Generating automatic masks... This may take a moment.\")\n",
        "\n",
        "# Generate masks automatically\n",
        "with torch.inference_mode(), torch.autocast(device.type, dtype=torch.bfloat16):\n",
        "    masks = mask_generator.generate(image)\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "# Original image\n",
        "axes[0].imshow(image)\n",
        "axes[0].set_title('Original Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Automatic segmentation\n",
        "axes[1].imshow(image)\n",
        "show_anns(masks, axes[1])\n",
        "axes[1].set_title(f'Automatic Segmentation ({len(masks)} masks)')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"‚úÖ Generated {len(masks)} automatic masks\")\n",
        "print(f\"üìä Mask areas: {[m['area'] for m in masks[:5]][:5]}...\")  # Show first 5 areas\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéâ Conclusion\n",
        "\n",
        "Congratulations! You've successfully set up and used SAM2 in Google Colab. Here's what you've learned:\n",
        "\n",
        "‚úÖ **Setup**: How to install SAM2 and its dependencies in Colab  \n",
        "‚úÖ **Image Segmentation**: Using point and box prompts for precise segmentation  \n",
        "‚úÖ **Automatic Masks**: Generating all possible masks automatically  \n",
        "‚úÖ **Helper Functions**: Building custom visualization and processing tools  \n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. **Experiment** with your own images\n",
        "2. **Try different models** to find the best balance for your use case\n",
        "3. **Combine prompts** for more precise segmentation\n",
        "4. **Build applications** using SAM2 as a foundation\n",
        "5. **Explore fine-tuning** for domain-specific tasks\n",
        "\n",
        "### üìö Resources:\n",
        "\n",
        "- [SAM2 Paper](https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/)\n",
        "- [GitHub Repository](https://github.com/facebookresearch/sam2)\n",
        "- [SAM2 Demo](https://sam2.metademolab.com/)\n",
        "- [Hugging Face Models](https://huggingface.co/models?search=facebook/sam2)\n",
        "\n",
        "### üí° Tips for Better Results:\n",
        "\n",
        "1. **Point Prompts**: Use positive points (label=1) to include areas, negative points (label=0) to exclude\n",
        "2. **Bounding Boxes**: Draw tight boxes around objects for better segmentation\n",
        "3. **Combined Prompts**: Mix points and boxes for optimal results\n",
        "4. **Model Selection**: Use tiny for speed, large for accuracy\n",
        "5. **Image Quality**: Higher resolution images generally give better results\n",
        "\n",
        "### üîß Troubleshooting:\n",
        "\n",
        "- If you encounter memory issues, try using a smaller model or reducing image resolution\n",
        "- For CUDA extension issues, the notebook is configured to skip them (functionality will be slightly limited but still work)\n",
        "- If segmentation quality is poor, try adjusting prompt placement or using multiple prompts\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Segmenting! üéØ**\n",
        "\n",
        "Upload your own images and experiment with different prompts to see SAM2 in action!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
